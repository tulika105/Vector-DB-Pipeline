# -*- coding: utf-8 -*-
"""Vector DB-tutorial(FAISS).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1USksQE6aAc_p-qw-qxzHUtQTc9UWRJRQ

**This tutorial is about collecting data to querying vector DB FAISS**

*Step 1: Install libraries*
"""

!pip install sentence-transformers faiss-cpu

"""*Step 2: Import libraries*"""

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

"""*Step 3: Collect data (sample data used here for demo)*"""

texts = [
    "The quick brown fox jumps over the lazy dog.",
    "Data science is an inter-disciplinary field.",
    "Artificial Intelligence and Machine Learning are transforming industries.",
    "Natural Language Processing is a subfield of AI.",
    "Python is a popular programming language."
] # list of sentences

"""*Step 4: Preprocessing data (skipping as of now)*

*Step 5: Convert data to vector embeddings*
"""

model = SentenceTransformer('paraphrase-MiniLM-L6-v2') # pre-trained sentence embedding model
embeddings = model.encode(texts) # each sentence is converted to 768 dimensional vector

"""*Step 6: Print the vector embeddings*"""

print(embeddings)

"""*Step 7: Initialize FAISS index using L2 distance (brute force search algorithm)*"""

embeddings = np.array(embeddings).astype('float32')
index = faiss.IndexFlatL2(embeddings.shape[1])

"""*Step 8: Add the embeddings*"""

index.add(embeddings)

"""*Step 9: Query the vector database FAISS*"""

query = 'what is machine learning?'
query_embedding = model.encode([query]) # Converting the query into vector

"""*Step 10: Perform a search in the FAISS index (retrieving the top 2 closest vectors)*"""

k = 2 # retrieving top 2 closest vector of the giving query vector
distance, indices = index.search(query_embedding,k)

"""*Step 11: Display the result*"""

print(f'query:{query}')
for i in range(k):
    print(f"Rank {i+1}:")
    print(f"Text: {texts[indices[0][i]]}")
    print(f"Distance: {distance[0][i]}")